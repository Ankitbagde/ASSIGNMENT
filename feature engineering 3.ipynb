{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d917af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling is a data normalization technique that scales the features of a dataset to a range between 0 and 1.\n",
    "It is used in data preprocessing to bring all features to a common scale, so that no single feature dominates the \n",
    "model's learning process.\n",
    "\n",
    "The Min-Max scaling can be easily implemented using the scikit-learn library in Python. We can use the MinMaxScaler \n",
    "class from the preprocessing module of scikit-learn to perform Min-Max scaling.\n",
    "\n",
    "Here's an example of how to use Min-Max scaling in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd116e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age    income\n",
      "0  0.00  0.000000\n",
      "1  0.25  0.142857\n",
      "2  0.50  0.428571\n",
      "3  0.75  0.714286\n",
      "4  1.00  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = {'age': [20, 25, 30, 35, 40],\n",
    "        'income': [25000, 30000, 40000, 50000, 60000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Instantiate the MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling to the dataframe\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Print the scaled dataframe\n",
    "print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858aff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique in feature scaling, also known as the \"Normalization\" technique, is a method to rescale \n",
    "the feature vector to have a length of 1. The process involves dividing each feature value by the L2-norm of the \n",
    "feature vector. This method is used to bring all feature vectors to the same scale and to remove the effect of\n",
    "magnitude from the feature vector.\n",
    "\n",
    "The Unit Vector technique can be easily implemented using the scikit-learn library in Python. We can use the\n",
    "Normalizer class from the preprocessing module of scikit-learn to perform Unit Vector scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6d1bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  income\n",
      "0  0.000800     1.0\n",
      "1  0.000833     1.0\n",
      "2  0.000750     1.0\n",
      "3  0.000700     1.0\n",
      "4  0.000667     1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onkibagde.619gmail.com/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but Normalizer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = {'age': [20, 25, 30, 35, 40],\n",
    "        'income': [25000, 30000, 40000, 50000, 60000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Instantiate the Normalizer object\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "# Apply Unit Vector scaling to the dataframe\n",
    "df_scaled = pd.DataFrame(normalizer.transform(df), columns=df.columns)\n",
    "\n",
    "# Print the scaled dataframe\n",
    "print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdff52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b082d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA, or Principal Component Analysis, is a statistical technique used for reducing the dimensionality of \n",
    "high-dimensional data while retaining most of its original variability. It works by transforming the original \n",
    "data into a lower-dimensional space, where the new dimensions are called principal components. These principal \n",
    "components are linear combinations of the original features, and each successive principal component captures\n",
    "as much of the remaining variability in the data as possible. The first principal component is the linear\n",
    "combination of the original features that captures the most variability in the data, the second principal\n",
    "component captures the next most variability, and so on.\n",
    "\n",
    "PCA is used for dimensionality reduction because it can reduce the number of features in a dataset while \n",
    "retaining most of the information present in the data. This can be useful in many applications, such as in \n",
    "image processing, where a large number of pixels can be compressed into a smaller number of principal components\n",
    "without losing much information. PCA is also commonly used in data visualization and exploratory data analysis to \n",
    "identify patterns in high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a829a89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92461872 0.05306648]\n",
      "[[-2.68412563  0.31939725]\n",
      " [-2.71414169 -0.17700123]\n",
      " [-2.88899057 -0.14494943]\n",
      " [-2.74534286 -0.31829898]\n",
      " [-2.72871654  0.32675451]\n",
      " [-2.28085963  0.74133045]\n",
      " [-2.82053775 -0.08946138]\n",
      " [-2.62614497  0.16338496]\n",
      " [-2.88638273 -0.57831175]\n",
      " [-2.6727558  -0.11377425]\n",
      " [-2.50694709  0.6450689 ]\n",
      " [-2.61275523  0.01472994]\n",
      " [-2.78610927 -0.235112  ]\n",
      " [-3.22380374 -0.51139459]\n",
      " [-2.64475039  1.17876464]\n",
      " [-2.38603903  1.33806233]\n",
      " [-2.62352788  0.81067951]\n",
      " [-2.64829671  0.31184914]\n",
      " [-2.19982032  0.87283904]\n",
      " [-2.5879864   0.51356031]\n",
      " [-2.31025622  0.39134594]\n",
      " [-2.54370523  0.43299606]\n",
      " [-3.21593942  0.13346807]\n",
      " [-2.30273318  0.09870885]\n",
      " [-2.35575405 -0.03728186]\n",
      " [-2.50666891 -0.14601688]\n",
      " [-2.46882007  0.13095149]\n",
      " [-2.56231991  0.36771886]\n",
      " [-2.63953472  0.31203998]\n",
      " [-2.63198939 -0.19696122]\n",
      " [-2.58739848 -0.20431849]\n",
      " [-2.4099325   0.41092426]\n",
      " [-2.64886233  0.81336382]\n",
      " [-2.59873675  1.09314576]\n",
      " [-2.63692688 -0.12132235]\n",
      " [-2.86624165  0.06936447]\n",
      " [-2.62523805  0.59937002]\n",
      " [-2.80068412  0.26864374]\n",
      " [-2.98050204 -0.48795834]\n",
      " [-2.59000631  0.22904384]\n",
      " [-2.77010243  0.26352753]\n",
      " [-2.84936871 -0.94096057]\n",
      " [-2.99740655 -0.34192606]\n",
      " [-2.40561449  0.18887143]\n",
      " [-2.20948924  0.43666314]\n",
      " [-2.71445143 -0.2502082 ]\n",
      " [-2.53814826  0.50377114]\n",
      " [-2.83946217 -0.22794557]\n",
      " [-2.54308575  0.57941002]\n",
      " [-2.70335978  0.10770608]\n",
      " [ 1.28482569  0.68516047]\n",
      " [ 0.93248853  0.31833364]\n",
      " [ 1.46430232  0.50426282]\n",
      " [ 0.18331772 -0.82795901]\n",
      " [ 1.08810326  0.07459068]\n",
      " [ 0.64166908 -0.41824687]\n",
      " [ 1.09506066  0.28346827]\n",
      " [-0.74912267 -1.00489096]\n",
      " [ 1.04413183  0.2283619 ]\n",
      " [-0.0087454  -0.72308191]\n",
      " [-0.50784088 -1.26597119]\n",
      " [ 0.51169856 -0.10398124]\n",
      " [ 0.26497651 -0.55003646]\n",
      " [ 0.98493451 -0.12481785]\n",
      " [-0.17392537 -0.25485421]\n",
      " [ 0.92786078  0.46717949]\n",
      " [ 0.66028376 -0.35296967]\n",
      " [ 0.23610499 -0.33361077]\n",
      " [ 0.94473373 -0.54314555]\n",
      " [ 0.04522698 -0.58383438]\n",
      " [ 1.11628318 -0.08461685]\n",
      " [ 0.35788842 -0.06892503]\n",
      " [ 1.29818388 -0.32778731]\n",
      " [ 0.92172892 -0.18273779]\n",
      " [ 0.71485333  0.14905594]\n",
      " [ 0.90017437  0.32850447]\n",
      " [ 1.33202444  0.24444088]\n",
      " [ 1.55780216  0.26749545]\n",
      " [ 0.81329065 -0.1633503 ]\n",
      " [-0.30558378 -0.36826219]\n",
      " [-0.06812649 -0.70517213]\n",
      " [-0.18962247 -0.68028676]\n",
      " [ 0.13642871 -0.31403244]\n",
      " [ 1.38002644 -0.42095429]\n",
      " [ 0.58800644 -0.48428742]\n",
      " [ 0.80685831  0.19418231]\n",
      " [ 1.22069088  0.40761959]\n",
      " [ 0.81509524 -0.37203706]\n",
      " [ 0.24595768 -0.2685244 ]\n",
      " [ 0.16641322 -0.68192672]\n",
      " [ 0.46480029 -0.67071154]\n",
      " [ 0.8908152  -0.03446444]\n",
      " [ 0.23054802 -0.40438585]\n",
      " [-0.70453176 -1.01224823]\n",
      " [ 0.35698149 -0.50491009]\n",
      " [ 0.33193448 -0.21265468]\n",
      " [ 0.37621565 -0.29321893]\n",
      " [ 0.64257601  0.01773819]\n",
      " [-0.90646986 -0.75609337]\n",
      " [ 0.29900084 -0.34889781]\n",
      " [ 2.53119273 -0.00984911]\n",
      " [ 1.41523588 -0.57491635]\n",
      " [ 2.61667602  0.34390315]\n",
      " [ 1.97153105 -0.1797279 ]\n",
      " [ 2.35000592 -0.04026095]\n",
      " [ 3.39703874  0.55083667]\n",
      " [ 0.52123224 -1.19275873]\n",
      " [ 2.93258707  0.3555    ]\n",
      " [ 2.32122882 -0.2438315 ]\n",
      " [ 2.91675097  0.78279195]\n",
      " [ 1.66177415  0.24222841]\n",
      " [ 1.80340195 -0.21563762]\n",
      " [ 2.1655918   0.21627559]\n",
      " [ 1.34616358 -0.77681835]\n",
      " [ 1.58592822 -0.53964071]\n",
      " [ 1.90445637  0.11925069]\n",
      " [ 1.94968906  0.04194326]\n",
      " [ 3.48705536  1.17573933]\n",
      " [ 3.79564542  0.25732297]\n",
      " [ 1.30079171 -0.76114964]\n",
      " [ 2.42781791  0.37819601]\n",
      " [ 1.19900111 -0.60609153]\n",
      " [ 3.49992004  0.4606741 ]\n",
      " [ 1.38876613 -0.20439933]\n",
      " [ 2.2754305   0.33499061]\n",
      " [ 2.61409047  0.56090136]\n",
      " [ 1.25850816 -0.17970479]\n",
      " [ 1.29113206 -0.11666865]\n",
      " [ 2.12360872 -0.20972948]\n",
      " [ 2.38800302  0.4646398 ]\n",
      " [ 2.84167278  0.37526917]\n",
      " [ 3.23067366  1.37416509]\n",
      " [ 2.15943764 -0.21727758]\n",
      " [ 1.44416124 -0.14341341]\n",
      " [ 1.78129481 -0.49990168]\n",
      " [ 3.07649993  0.68808568]\n",
      " [ 2.14424331  0.1400642 ]\n",
      " [ 1.90509815  0.04930053]\n",
      " [ 1.16932634 -0.16499026]\n",
      " [ 2.10761114  0.37228787]\n",
      " [ 2.31415471  0.18365128]\n",
      " [ 1.9222678   0.40920347]\n",
      " [ 1.41523588 -0.57491635]\n",
      " [ 2.56301338  0.2778626 ]\n",
      " [ 2.41874618  0.3047982 ]\n",
      " [ 1.94410979  0.1875323 ]\n",
      " [ 1.52716661 -0.37531698]\n",
      " [ 1.76434572  0.07885885]\n",
      " [ 1.90094161  0.11662796]\n",
      " [ 1.39018886 -0.28266094]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Instantiate the PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the dataset\n",
    "X_pca = pca.fit_transform(iris.data)\n",
    "\n",
    "# Print the explained variance ratio of the principal components\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the transformed dataset\n",
    "print(X_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f2aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA and feature extraction are closely related concepts. In fact, PCA can be used as a feature extraction technique.\n",
    "\n",
    "Feature extraction is the process of transforming raw data into a set of features that are more informative and\n",
    "relevant for a particular task. Feature extraction is commonly used in machine learning and computer vision \n",
    "applications to reduce the dimensionality of the data and extract meaningful features that can be used for \n",
    "classification, clustering, or other tasks.\n",
    "\n",
    "PCA is a technique for dimensionality reduction that can be used for feature extraction. By identifying the\n",
    "principal components of a dataset, PCA can extract a set of features that capture most of the variability in \n",
    "the data. These features can be used for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Instantiate the PCA object\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "# Apply PCA to the dataset\n",
    "X_pca = pca.fit_transform(digits.data)\n",
    "\n",
    "# Print the shape of the transformed dataset\n",
    "print(X_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8dd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d62f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a recommendation system for a food delivery service, it's important to preprocess the data before using it for \n",
    "modeling. One common technique for preprocessing numerical features is Min-Max scaling.\n",
    "\n",
    "Min-Max scaling transforms the features so that they have a minimum value of 0 and a maximum value of 1. \n",
    "This technique can help to normalize the data and make it easier to compare and analyze the different features.\n",
    "\n",
    "To use Min-Max scaling to preprocess the features in the food delivery service dataset, you would follow these steps:\n",
    "\n",
    "Identify the numerical features that need to be scaled. In this case, the features might include price, rating,\n",
    "and delivery time.\n",
    "\n",
    "Calculate the minimum and maximum values for each feature in the dataset.\n",
    "\n",
    "Use the formula (x - min) / (max - min) to scale each feature, where x is the original value, min is the minimum \n",
    "value for the feature, and max is the maximum value for the feature.\n",
    "\n",
    "Replace the original values with the scaled values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8cea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the food delivery service dataset\n",
    "df = pd.read_csv('food_delivery_service.csv')\n",
    "\n",
    "# Identify the numerical features that need to be scaled\n",
    "features_to_scale = ['price', 'rating', 'delivery_time']\n",
    "\n",
    "# Instantiate the MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling to the numerical features\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "\n",
    "# Print the preprocessed dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550973ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af620ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "When working with a large dataset containing many features, such as in the case of predicting stock prices, it can \n",
    "be helpful to use dimensionality reduction techniques like PCA to reduce the number of features and simplify the data.\n",
    "\n",
    "PCA (Principal Component Analysis) is a technique that can be used to identify patterns in data and reduce the \n",
    "dimensionality of the dataset by transforming it into a new set of variables called principal components. \n",
    "These principal components are linear combinations of the original features, and each component captures a certain amount of the variation in the data.\n",
    "\n",
    "Here's how you could use PCA to reduce the dimensionality of a dataset for predicting stock prices:\n",
    "\n",
    "First, preprocess the data by standardizing the features so that they have a mean of 0 and a standard deviation of 1.\n",
    "This is important for PCA, as it assumes that the data is normally distributed and standardized.\n",
    "\n",
    "Use PCA to transform the data into a new set of principal components. The number of principal components you choose \n",
    "will depend on the amount of variance you want to preserve in the data. You can use the scikit-learn library in Python to perform PCA on the data:\n",
    "\n",
    "In the above example, we first loaded the dataset and separated the features from the target variable. We then \n",
    "standardized the features using the StandardScaler function from scikit-learn, and applied PCA with two components\n",
    "using the PCA function from scikit-learn. We then fit the PCA model to the standardized features using the \n",
    "fit_transform method.\n",
    "\n",
    "After obtaining the new set of principal components, you can use them to train your prediction model.\n",
    "Each principal component captures a certain amount of the variation in the original data, so you can choose \n",
    "the principal components that explain the most variation in the data.\n",
    "PCA can help to reduce the dimensionality of the dataset and eliminate redundant features that may not be \n",
    "informative for predicting stock prices. By transforming the data into a new set of principal components, \n",
    "you can simplify the data and improve the performance of your prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c50f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset and separate the features from the target variable\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA on the standardized features\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93987bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4c1f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaled_data = scaler.fit_transform([data])\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8863f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "The number of principal components to retain in PCA depends on the amount of variance we want to preserve in the\n",
    "original data. We want to retain as much variance as possible while reducing the dimensionality of the data. \n",
    "A commonly used criterion is to retain enough principal components to explain a certain percentage of the variance \n",
    "in the data.\n",
    "\n",
    "To determine how many principal components to retain in this example, we would need to calculate the percentage of \n",
    "variance explained by each principal component. This can be done by looking at the eigenvalues of the covariance \n",
    "matrix generated by the data.\n",
    "\n",
    "Without knowing the specific dataset, it's difficult to give an exact answer, but we could start by looking at the\n",
    "scree plot, which shows the proportion of variance explained by each principal component. We would then choose the \n",
    "number of principal components that explain a large portion of the variance in the data, while also keeping in mind \n",
    "the practical implications of using a lower-dimensional representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a2d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
